# <p align="center">Machine Translation before Transformers
</p>

Breakthrough in NLP is the advent of Transformer on AI research community brought by the paper [Attention is all you need](https://arxiv.org/abs/1706.03762) by Vaswani et al. From then Transformer became part and parcel of NLP on different downstream task like Text Summarization, Machine Translation, Name Entity Recognition, Question Answering and also now its invading the field of [Computer Vision](https://arxiv.org/abs/2010.11929).

This repo focuses on the *Machine Translation with LSTM and different techniques* revolving around it.
This repo is created using Pytorch and some available NLP library like spacy etc.
But Due to Low Resource at the moment, trained model is not available.
The Notebook file of the codebase will be soon made available to [my kaggle account](https://www.kaggle.com/ankan1998)

<hr>

![LSTM](https://www.researchgate.net/profile/Xuan_Hien_Le2/publication/334268507/figure/fig8/AS:788364231987201@1564972088814/The-structure-of-the-Long-Short-Term-Memory-LSTM-neural-network-Reproduced-from-Yan.png)

<hr>

## *Requirements*
* torch = 1.6.0
* torchtext = 0.6.0
* spacy
* numpy
* PIL
* opencv


### *TASKLIST*
* [ ] Machine Translation with Bidirectional LSTM
    * [x] Data Preparation
    * [x] Encoder
    * [x] Decoder
    * [x] Seq2Seq
    * [ ] Training Script
    * [ ] Inference Script
* [ ] Learning Phrase Representations using seq2seq Statistical Machine Translation
* [ ] Machine Translation with Attention
* [ ] Machine Translation with Convolutional Model







